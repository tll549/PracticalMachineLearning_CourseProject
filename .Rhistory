data(mtcars)
fit3 <- lm(mpg ~ wt, data = mtcars)
predict(fit3, data.frame(wt=mean(mtcars$wt)), interval = ("confidence"))[2]
fit3 <- lm(mtcars$mpg ~ mtcars$wt)
predict(fit3, data.frame(wt=mean(mtcars$wt)), interval = ("confidence"))[2]
sumCoef[1,1] + c(-1, 1) * qt(.95, df = fit3$df) * sumCoef[1, 2]
mean(mtcars$mwt)
mean(mtcars$wt)
34 - mean(mtcars$wt)
sumCoef[1,1] + c(-1, 1) * qt(.95, df = fit3$df) * sumCoef[1, 2] * mean(mtcars$wt)
sumCoef[1,1] + c(-1, 1) * qt(.95, df = fit3$df) * sumCoef[1, 2]
sumCoef[1,1]
newdata
mean(mtcars$wt)
predict(fit3, newdata)
predict(fit3, mean(mtcars$wt))
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
fit <- lm(y ~ x);
summary(fit)$coefficients
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
data(mtcars)
fit3 <- lm(mtcars$mpg ~ mtcars$wt)
mean(mtcars$wt)
prediction(m = mean(mtcars$wt))
predict(m = mean(mtcars$wt))
predict(fit3, data.frame(m = mean(mtcars$wt)))
data.frame(m = mean(mtcars$wt)
)
predict(fit3, data.frame(m = mean(mtcars$wt)), interval = ("confidence"))
data.frame(m = mean(mtcars$wt))
data.frame(x=mean(x))
x<-mtcars$wt
y<-mtcars$mpg
data.frame(x=mean(x))
mtcars$wt
x
identical(x, mtcars$wt)
identical(lm(y ~ x), lm(mtcars$mpg ~ mtcars$wt))
identical(lm(y ~ x), lm(mpg ~ wt, mtcars))
identical(lm(y ~ x), lm(mpg ~ wt, data = mtcars))
identical(lm(mtcars$mpg ~ mtcars$wt), lm(mtcars$mpg ~ mtcars$wt))
identical(y, mtcars$mpg)
identical(x, mtcars$wt)
identical(y+x, mtcars$wt+mtcars$mpg)
identical(lm(y ~ x), lm(mpg ~ wt, data = mtcars))
lm(mtcars$mpg ~ mtcars$wt)
lm(y ~ x)
predict(fit3, data.frame(m = mean(mtcars$wt)))
predict(fit,data.frame(x=mean(x)), interval="confidence")
fit3 <- lm(mtcars$mpg ~ mtcars$wt)
fit3_ <- lm(y ~ x)
predict(fit3, data.frame(m = mean(mtcars$wt)))
predict(fit3_, data.frame(m = mean(mtcars$wt)))
fit3 <- lm(mtcars$mpg ~ mtcars$wt)
x<-mtcars$wt
y<-mtcars$mpg
fit3_ <- lm(y ~ x)
predict(fit3, data.frame(m = mean(mtcars$wt)))
predict(fit3_, data.frame(m = mean(mtcars$wt)))
predict(fit3, data.frame(m = mean(mtcars$wt)), interval="confidence")
predict(fit3_, data.frame(m = mean(mtcars$wt)), interval="confidence")
predict(fit3, data.frame(m = mean(x)))
predict(fit3_, data.frame(m = mean(x)))
predict(fit3_, data.frame(m = mean(x)), interval = "confidence")
predict(fit,data.frame(x=mean(x)), interval="confidence")
predict(fit3_, data.frame(x = mean(x)), interval = "confidence")
predict(fit3, data.frame(x = mean(mtcars$wt)))
predict(fit3_, data.frame(x = mean(mtcars$wt)))
predict(fit3_, data.frame(x = mean(mtcars$wt)))
predict(fit3, data.frame(mtcars$wt = mean(mtcars$wt)))
predict(fit3, data.frame(wt = mean(mtcars$wt)))
predict(fit3_, data.frame(x = mean(mtcars$wt)))
rm(list = ls())
data(mtcars)
x<-mtcars$wt
y<-mtcars$mpg
fit3 <- lm(y ~ x)
predict(fit3, data.frame(x = mean(x)), interval = "confidence")
?mtcars
mtcars$wt
predict(fit3, data.fame(x = 3), interval = "confidence")
predict(fit3, data.frame(x = 3), interval = "confidence")
predict(fit3, data.frame(x = 3))
predict(fit3, data.frame(x = 3), interval = "interval")
predict(fit3, data.frame(x = 3), interval = "prediction")
?predict
predict(fit3, data.frame(x = 3), interval = "confidence")
predict(fit3, data.frame(x = 2), interval = "confidence")
qt(0.025, n-2))
qt(0.025, n-2)
qt(0.025, fit3$df)
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
fit <- lm(y ~ x);
summary(fit)$coefficients
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
library(ggplot2)
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"
g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
g = g + geom_line()
g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
library(ggplot2)
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"
g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
g = g + geom_line()
g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
library(ggplot2)
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"
g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
g = g + geom_line()
g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
p1
sumCoef[2,1] - qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
(sumCoef[2,1] - qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
fit3$coefficients
summary(fit3)
summary(fit3)$Coefficients
summary(fit3)$coefficient
(coef[2,1] - qt(.975, df = fit3$df) * coef[2, 2]) * 2000
coef <- summary(fit3)$coefficient
(coef[2,1] - qt(.975, df = fit3$df) * coef[2, 2]) * 2000
(coef[2,1] - qt(.975, df = fit3$df) * coef[2, 2]) * 2
summary(fit3)
newdata <- data.frame(x = xVals)
p1 <- predict(fit, newdata, interval = ("confidence"))
p2 <- predict(fit, newdata, interval = ("prediction"))
plot(x, y, frame=FALSE,xlab="Carat",ylab="Dollars",pch=21,col="black", bg="lightblue", cex=2)
abline(fit, lwd = 2)
lines(xVals, p1[,2]); lines(xVals, p1[,3])
lines(xVals, p2[,2]); lines(xVals, p2[,3])
source('~/.active-rstudio-document', echo=TRUE)
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
fit <- lm(y ~ x);
summary(fit)$coefficients
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
library(ggplot2)
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"
g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
g = g + geom_line()
g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
newdata <- data.frame(x = xVals)
p1 <- predict(fit, newdata, interval = ("confidence"))
p2 <- predict(fit, newdata, interval = ("prediction"))
plot(x, y, frame=FALSE,xlab="Carat",ylab="Dollars",pch=21,col="black", bg="lightblue", cex=2)
abline(fit, lwd = 2)
lines(xVals, p1[,2]); lines(xVals, p1[,3])
lines(xVals, p2[,2]); lines(xVals, p2[,3])
?lm
data(mtcars)
x<-mtcars$wt
y<-mtcars$mpg
fit3 <- lm(y ~ x)
predict(fit3, data.frame(x = mean(x)), interval = "confidence")
predict(fit3, data.frame(x = 3), interval = "prediction")
coef <- summary(fit3)$coefficient
(coef[2,1] - qt(.975, df = fit3$df) * coef[2, 2]) * 2
fit9 <- lm(y ~ 1) # just an intercept
anova(fit3)
anova(fit9)
278/1126
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
head(spam)
table(spam)
table(spam$type)
install.packages("carat")
install.packages('caret')
install.packages("AppliedPredictiveModeling")
2
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
rm(list = ls())
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
?createDataPartition
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(caret)
head(concrete)
qplot(concrete$Superplasticizer)
qplot(log(oncrete$Superplasticizer))
qplot(log(concrete$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Find all the predictor vari
head(training)
library(dplyr)
?select
names(training)
training_IL <- select(training, starts_with("IL"))
names(training_IL)
preProc <- preProcess(training_IL, method = "pca")
preProc
summary(preProc)
?preProcess
confusionMatrix.train(traingin_IL)
confusionMatrix.train(training_IL)
preProc <- preProcess(training_IL, method = "pca", thresh = .9)
preProc
train_data <- select(training, diagnosis, starts_with("IL"))
names(train_data)
modFit <- train(diagnosis ~ ., data = train_data, method = "glm")
modFit
preProc <- preProcess(training_IL, method = "pca", thresh = .8)
preProc
predict(modFit, train_data$diagnosis)
non_pca_result <- confusionMatrix(train_data$diagnosis, predict(non_pca_model, train_data$diagnosis))
non_pca_result <- confusionMatrix(train_data$diagnosis, predict(modFit, train_data$diagnosis))
train_data <- select(training, diagnosis, starts_with("IL"))
modFit <- train(diagnosis ~ ., data = train_data, method = "glm")
non_pca_result <- confusionMatrix(train_data$diagnosis, predict(modFit, train_data$diagnosis))
head(train_data)
test_data <- select(testing, diagnosis, starts_with("IL"))
modFit <- train(diagnosis ~ ., data = train_data, method = "glm")
predict(modFit, newdata = test_data)
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
##
## Attaching package: 'e1071'
##
## The following object is masked from 'package:Hmisc':
##
##     impute
predictions <- predict(modelFit, newdata = testing)
## get the confustion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
train_data <- select(training, diagnosis, starts_with("IL"))
test_data <- select(testing, diagnosis, starts_with("IL"))
modFit <- train(diagnosis ~ ., data = train_data, method = "glm")
predict(modFit, newdata = test_data)
predict(modFit, data = test_data)
# compute the model with non_pca predictors
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
# apply the non pca model on the testing set and check the accuracy
non_pca_result <- confusionMatrix(new_testing[, 13], predict(non_pca_model, new_testing[, -13]))
non_pca_result
# extract new training and testing sets
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
suppressMessages(library(dplyr))
new_training <- training[, c(names(training)[IL_col_idx], "diagnosis")]
names(new_training)
##  [1] "IL_11"         "IL_13"         "IL_16"         "IL_17E"
##  [5] "IL_1alpha"     "IL_3"          "IL_4"          "IL_5"
##  [9] "IL_6"          "IL_6_Receptor" "IL_7"          "IL_8"
## [13] "diagnosis"
IL_col_idx <- grep("^[Ii][Ll].*", names(testing))
suppressMessages(library(dplyr))
new_testing <- testing[, c(names(testing)[IL_col_idx], "diagnosis")]
names(new_testing)
##  [1] "IL_11"         "IL_13"         "IL_16"         "IL_17E"
##  [5] "IL_1alpha"     "IL_3"          "IL_4"          "IL_5"
##  [9] "IL_6"          "IL_6_Receptor" "IL_7"          "IL_8"
## [13] "diagnosis"
# compute the model with non_pca predictors
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
# apply the non pca model on the testing set and check the accuracy
non_pca_result <- confusionMatrix(new_testing[, 13], predict(non_pca_model, new_testing[, -13]))
non_pca_result
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
suppressMessages(library(dplyr))
new_training <- training[, c(names(training)[IL_col_idx], "diagnosis")]
names(new_training)
train_data
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
suppressMessages(library(dplyr))
new_training <- training[, c(names(training)[IL_col_idx], "diagnosis")]
names(new_training)
names(train_data)
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
non_pca_result <- confusionMatrix(new_testing[, 13], predict(non_pca_model, new_testing[, -13]))
non_pca_result
modFit <- train(diagnosis ~ ., data = train_data, method = "glm")
confusionMatrix(test_data$diagnosis, predict(modFit, test_data$diagnosis))
train_data <- select(training, starts_with("IL"), diagnosis)
names(train_data)
train_data <- select(training, starts_with("IL"), diagnosis)
test_data <- select(testing, starts_with("IL"), diagnosis)
modFit <- train(diagnosis ~ ., data = train_data, method = "glm")
predict(modFit, data = test_data)
confusionMatrix(test_data$diagnosis, predict(modFit, test_data$diagnosis))
predict(preProc, test_data$diagnosis)
preProc <- preProcess(train_data[, -13], method = "pca", thresh = .8)
predict(preProc, test_data$diagnosis)
predict(preProc, test_data[, 13])
library(slidify)
install_github('slidifyLibraries', 'ramnathv')
library(devtools)
install_github('slidifyLibraries', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv/slidifyLibraries')
install_github('ramnathv/slidifyLibraries')
download.packages("rattle")
download.packages("rattle", destdir = "~/")
download.packages("rattle", method = "curl")
download.packages("rattle", destdir = "~/", method = "curl")
library(rattle)
install.packages("RGtk2")
install.packages("rattle")
library(rattle)
q()
prp(modFit)
install.packages("caTools")
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
prp(modFit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
library(dplyr)
test <- filter(segmentationOriginal, Case == "Test")
train <- filter(segmentationOriginal, Case == "Train")
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings.
set.seed(125)
modFit <- train(Class ~ ., data = train, method = "rpart")
# 3. In the final model what would be the final model prediction for cases with the following variable values:
modFit$finalModel
library(rattle)
fancyRpartPlot(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
prp(modFit)
library(ElemStatLearn)
download.packages("ElemStatLearn")
rm(list=ls())
download.packages("ElemStatLearn")
setwd("/Users/T/Dropbox/Coursera/Data Science S/8 Practical Machine Learning/project writeup")
if (!file.exists("pml-training.csv"))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv", method = "curl")
if (!file.exists("pml-testing.csv"))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv", method = "curl")
train <- read.csv("pml-training.csv", na.strings = c("#DIV/0!", "", "NA"))
test <- read.csv("pml-testing.csv", na.strings = c("#DIV/0!", "", "NA"))
# check the dimensions of two dataset
dim(train)
dim(test)
library(caret)
set.seed(555)
inTrain <- createDataPartition(train$classe, p = 0.75, list = FALSE)
training <- train[inTrain,]
testing <- train[-inTrain,]
# check the dimensions
dim(training)
dim(testing)
# to see the results, uncomment this line
# nearZeroVar(training, saveMetrics = T)
# removing variables
nsv <- nearZeroVar(training)
trainingSelected <- training[, -nsv]
dim(trainingSelected)
library(dplyr)
trainingSelected <- select(trainingSelected, -(X:num_window))
dim(trainingSelected)
# to see these variables, uncomment this line
# colSums(is.na(trainingSelected))
# remove these variables
trainingSelectedRmna <- trainingSelected[, colSums(is.na(trainingSelected)) == 0]
dim(trainingSelectedRmna)
library(randomForest)
RFmodel <- randomForest(classe~., data = trainingSelectedRmna, method = "class")
RFmodel
?MDSplot
MDSplot(RFmodel, training$classe)
MDSplot(RFmodel, training$classe, proximity = T)
MDSplot(RFmodel, training$classe, proximity = F)
example("randomForest")
?getTree
getTree(RFmodel, 1)
getTree(RFmodel)
prediction <- predict(RFmodel, testing, type = "class")
cm <- confusionMatrix(prediction, testing$classe)
cm
prediction
summary(prediction)
sum(prediction != testing$classe)
sum(prediction != testing$classe) / length(testing$classe)
cm
cm
cm$positive
cm$overall
cm$overall[1]
